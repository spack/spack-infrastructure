---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: kube-prometheus-stack-spack-custom-alerts
  namespace: monitoring
  labels:
    app: kube-prometheus-stack
    release: kube-prometheus-stack
spec:
  groups:
   - name: spack-custom-alerts
     rules:

       - alert: GitlabPipelinePodStuck
         annotations:
           description: '{{ $labels.pod }} has had no activity for more than 5 minutes.'
           runbook_url: 'TODO'
           summary: 'The pod appears to be stuck.  No CPU, RAM, or Network activity for the last 5 minutes.'
         expr: node_namespace_pod_name:pipeline_stuck_pods_info == 1
         for: 5m
         labels:
           group: gitlab_webservice_error_rate
           severity: info
           namespace: monitoring
           source_namespace: "{{ $labels.namespace }}"

       - alert: GitLabWebServiceErrorRate5Percent
         annotations:
           description: 'GitLab Web Service has been seeing a 5% error rate for the last 5 minutes'
           runbook_url: 'TODO'
           summary: 'The GitLab web service has seen 4XX/5XX responses for at least 5% of requests over the last 5 minutes.'
         expr: ingress_error_rate:gitlab_webservice_default <= 95.0
         for: 5m
         labels:
           group: gitlab_webservice_error_rate
           severity: warning
           namespace: monitoring
           source_namespace: "{{ $labels.namespace }}"


       - alert: GitLabWebServiceErrorRate10Percent
         annotations:
           description: 'GitLab Web Service has been seeing a 10% error rate for the last 2 minutes'
           runbook_url: 'TODO'
           summary: 'The GitLab web service has seen 4XX/5XX responses for at least 10% of requests over the last 2 minutes.'
         expr: ingress_error_rate:gitlab_webservice_default <= 90.0
         for: 2m
         labels:
           group: gitlab_webservice_error_rate
           severity: critical
           namespace: monitoring
           source_namespace: "{{ $labels.namespace }}"

       - alert: ContainerOOMKilled
         annotations:
           description: 'Container {{ $labels.container }} in pod {{ $labels.namespace }}/{{ $labels.pod }} has been OOMKilled {{ $value }} times in the last 10 minutes. LABELS = {{ $labels }}'
           runbook_url: 'TODO'
           summary: 'A container was OOMKilled'
         expr: (kube_pod_container_status_restarts_total - kube_pod_container_status_restarts_total offset 10m >= 1) and ignoring (reason) min_over_time(kube_pod_container_status_last_terminated_reason{reason="OOMKilled"}[10m]) == 1
         for: 0m
         labels:
           group: memory_usage_exceeded
           severity: critical
           namespace: monitoring
           source_namespace: "{{ $labels.namespace }}"

      #  - alert: ContainerMemoryUsageAbove80
      #    expr: (sum(container_memory_working_set_bytes{name!=""}) BY (instance, name) / sum(container_spec_memory_limit_bytes > 0) BY (instance, name) * 100) > 80
      #    for: 2m
      #    labels:
      #      group: memory_usage_exceeded
      #      severity: warning
      #      namespace: monitoring
      #      source_namespace: "{{ $labels.namespace }}"
      #    annotations:
      #      summary: 'Container Memory usage has been exceeding 80% for at least 2 minutes.'
      #      description: "Container {{ $labels.container }} memory usage is above 80% ({{ $value }}). LABELS = {{ $labels }}"
