---
apiVersion: source.toolkit.fluxcd.io/v1beta2
kind: HelmRepository
metadata:
  name: runner-spack-package-signing
  namespace: gitlab
spec:
  interval: 10m
  url: https://charts.gitlab.io

---
apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: runner-spack-package-signing
  namespace: gitlab
spec:
  interval: 10m
  chart:
    spec:
      chart: gitlab-runner
      version: 0.60.0 # gitlab-runner@16.7.0
      sourceRef:
        kind: HelmRepository
        name: runner-spack-package-signing

  valuesFrom:
    # See terraform/modules/sentry/sentry.tf
    - kind: ConfigMap
      name: gitlab-runner-sentry-config
      valuesKey: values.yaml

  values:
    imagePullPolicy: IfNotPresent
    replicas: 1

    gitlabUrl: "https://gitlab.spack.io/"
    unregisterRunners: true
    terminationGracePeriodSeconds: 21600 # six hours
    concurrent: 20
    checkInterval: 30
    # preEntrypointScript: |
    #   echo "Hello, from large-pub runner"

    metrics:
      enabled: true

    rbac:
      serviceAccountName: runner

    runners:
      config: |
        [[runners]]
          pre_build_script = """
          echo 'Executing Spack pre-build setup script'

          for cmd in "${PY3:-}" python3 python; do
            if command -v > /dev/null "$cmd"; then
              export PY3="$(command -v "$cmd")"
              break
            fi
          done

          if [ -z "${PY3:-}" ]; then
            echo "Unable to find python3 executable"
            exit 1
          fi

          $PY3 -c "import urllib.request;urllib.request.urlretrieve('https://raw.githubusercontent.com/spack/spack-infrastructure/main/scripts/gitlab_runner_pre_build/pre_build.py', 'pre_build.py')"
          $PY3 pre_build.py > envvars

          . ./envvars
          rm -f envvars
          unset GITLAB_OIDC_TOKEN
          """

          output_limit = 10240
          environment = ["FF_GITLAB_REGISTRY_HELPER_IMAGE=1"]
          [runners.kubernetes]
            privileged = false
            helper_memory_request = "512M"

            cpu_request = "10"
            cpu_limit = "10"
            memory_request = "8G"
            memory_limit = "64G"
            namespace = "pipeline"
            poll_timeout = 600  # ten minutes
            service_account = "notary"

            # TODO Change to actual image before merge
            allowed_images = ["ghcr.io/spack/notary:*"]
            allowed_services = [""]

            # TODO Set up security contexts with the image
            # pod_security_context =
            # helper_container_security_context =
            # build_container_security_context =
            #
            # TODO what capabilities can we drop from the container?
            # cap_drop =

            [runners.kubernetes.affinity]
              [runners.kubernetes.affinity.node_affinity]

              # Schedule this pod on any node with x86_64 >= v3
              [runners.kubernetes.affinity.node_affinity.required_during_scheduling_ignored_during_execution]
                [[runners.kubernetes.affinity.node_affinity.required_during_scheduling_ignored_during_execution.node_selector_terms]]
                  [[runners.kubernetes.affinity.node_affinity.required_during_scheduling_ignored_during_execution.node_selector_terms.match_expressions]]
                      key = "spack.io/x86_64"
                      operator = "In"
                      values = ["v3", "v4"]
                  [[runners.kubernetes.affinity.node_affinity.required_during_scheduling_ignored_during_execution.node_selector_terms.match_expressions]]
                      key = "spack.io/pipeline"
                      operator = "Exists"

              # Weight this pod towards x86-64-v3 nodes
              [[runners.kubernetes.affinity.node_affinity.preferred_during_scheduling_ignored_during_execution]]
                  weight = 2
                  [[runners.kubernetes.affinity.node_affinity.preferred_during_scheduling_ignored_during_execution.preference.match_expressions]]
                    key = "spack.io/x86_64"
                    operator = "In"
                    values = ["v3"]
              [[runners.kubernetes.affinity.node_affinity.preferred_during_scheduling_ignored_during_execution]]
                  weight = 1
                  [[runners.kubernetes.affinity.node_affinity.preferred_during_scheduling_ignored_during_execution.preference.match_expressions]]
                    key = "spack.io/x86_64"
                    operator = "In"
                    values = ["v4"]

              # Place pod close to other pipeline pods if possible ("pack" the pods tightly)
              # This takes precedence over the above weights, prioritizing pod packing
              # Docs: https://docs.gitlab.com/runner/executors/kubernetes.html#define-nodes-where-pods-are-scheduled
              [runners.kubernetes.affinity.pod_affinity]
                [[runners.kubernetes.affinity.pod_affinity.preferred_during_scheduling_ignored_during_execution]]
                weight = 4
                [runners.kubernetes.affinity.pod_affinity.preferred_during_scheduling_ignored_during_execution.pod_affinity_term]
                  topology_key = "topology.kubernetes.io/zone"
                  [runners.kubernetes.affinity.pod_affinity.preferred_during_scheduling_ignored_during_execution.pod_affinity_term.label_selector]
                    [[runners.kubernetes.affinity.pod_affinity.preferred_during_scheduling_ignored_during_execution.pod_affinity_term.label_selector.match_expressions]]
                      key = "spack.io/runner"
                      operator = "In"
                      values = ["true"]

            [runners.kubernetes.node_tolerations]
              "spack.io/runner-taint=true" = "NoSchedule"

            [runners.kubernetes.pod_annotations]
              "pod-cleanup.gitlab.com/ttl" = "7h"
              "fluentbit.io/exclude" = "true"
              "karpenter.sh/do-not-evict" = "true"
              "gitlab/ci_pipeline_url" = "$CI_PIPELINE_URL"
              "gitlab/ci_job_url" = "$CI_JOB_URL"
              "gitlab/ci_project_url" = "$CI_PROJECT_URL"
              "gitlab/ci_runner_description" = "$CI_RUNNER_DESCRIPTION"
              "gitlab/ci_job_id" = "$CI_JOB_ID"
            [runners.kubernetes.pod_labels]
              "spack.io/runner" = "true"
              "gitlab/ci_job_id" = "$CI_JOB_ID"
              "metrics/gitlab_ci_pipeline_id" = "$CI_PIPELINE_ID"
              "metrics/gitlab_ci_project_namespace" = "$CI_PROJECT_NAMESPACE"
              "metrics/gitlab_ci_project_name" = "$CI_PROJECT_NAME"
              "metrics/gitlab_ci_job_stage" = "$CI_JOB_STAGE"
              "metrics/gitlab_ci_commit_ref_name" = "$CI_COMMIT_REF_NAME"
              "metrics/spack_ci_stack_name" = "$SPACK_CI_STACK_NAME"
            [runners.kubernetes.node_selector]
              "kubernetes.io/arch" = "amd64"

            [[runners.kubernetes.volumes.secret]]
              name = "spack-signing-key-encrypted"
              mount_path = "/mnt/keys/signing"
              read_only = true

            [[runners.kubernetes.volumes.empty_dir]]
              name = "gnupg"
              mount_path = "/mnt/gnupg"
              medium = "Memory"

      # default image
      image: "ghcr.io/spack/notary:latest"
      imagePullPolicy: "always"
      tags: "spack,notary,aws,protected"

      locked: true
      protected: true
      runUntagged: false
      secret: spack-signing-runner-secret

      services: {}
      # cpuRequests: 50m
      # cpuLimit: 50m
      # memoryRequests: 144Mi
      # memoryLimit: 144Mi

      helpers: {}
      # cpuRequests: 50m
      # cpuLimit: 50m
      # memoryRequests: 144Mi
      # memoryLimit: 144Mi

    nodeSelector:
      spack.io/node-pool: base # pool for the runner

    podAnnotations:
      karpenter.sh/do-not-evict: true
